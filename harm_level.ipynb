{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6356940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: timm in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.15)\n",
      "Requirement already satisfied: textblob in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (78.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (8.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\startklar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Startklar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Startklar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Startklar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Startklar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Startklar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Startklar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch torchvision pandas scikit-learn matplotlib timm textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc8d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Startklar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#  Imports\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1eafb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mode = \"both\"  # or \"image\", or \"text\"\n",
    "dataset_dir = \"./data\"  # Update path to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d3ade8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "train_data = load_jsonl(os.path.join(dataset_dir, \"train.jsonl\"))\n",
    "dev_data   = load_jsonl(os.path.join(dataset_dir, \"dev.jsonl\"))\n",
    "\n",
    "df_all = pd.DataFrame(train_data + dev_data)\n",
    "df_all = df_all.sample(2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ea6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_harm_keywords = [\"kill\", \"rape\", \"nazi\", \"exterminate\", \"shoot\", \"burn\", \"die\", \"blood\", \"bomb\"]\n",
    "\n",
    "def classify_harm_level(row):\n",
    "    if row[\"label\"] == 0:\n",
    "        return 0  # Low\n",
    "    text = row[\"text\"].lower()\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if any(word in text for word in high_harm_keywords) or polarity < -0.5:\n",
    "        return 2  # High\n",
    "    return 1  # Medium\n",
    "\n",
    "df_all[\"harm_label\"] = df_all.apply(classify_harm_level, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2794ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      " harm_label\n",
      "0    1268\n",
      "1     630\n",
      "2     102\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Distribution:\\n\", df_all[\"harm_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20ffc01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Startklar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CLIP\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4d3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatefulMemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_img_dir, processor, mode=\"both\"):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.root_img_dir = root_img_dir\n",
    "        self.processor = processor\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        text = item[\"text\"]\n",
    "        label = int(item[\"harm_label\"])\n",
    "\n",
    "        image = None\n",
    "        if self.mode in [\"both\", \"image\"]:\n",
    "            image_file = item[\"img\"].replace(\"img/\", \"\")\n",
    "            image_path = os.path.join(self.root_img_dir, image_file)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.mode == \"both\":\n",
    "            inputs = self.processor(\n",
    "                text=[text], images=image, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                truncation=True, max_length=77, return_attention_mask=True\n",
    "            )\n",
    "        elif self.mode == \"image\":\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        elif self.mode == \"text\":\n",
    "            inputs = self.processor(\n",
    "                text=[text], return_tensors=\"pt\", padding=\"max_length\",\n",
    "                truncation=True, max_length=77, return_attention_mask=True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input_mode\")\n",
    "\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs, torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b2281b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_collate_fn(batch):\n",
    "    inputs_batch, labels = zip(*batch)\n",
    "    keys = inputs_batch[0].keys()\n",
    "    collated = {}\n",
    "\n",
    "    for key in keys:\n",
    "        tensors = [inputs[key] for inputs in inputs_batch]\n",
    "        if key in [\"input_ids\", \"attention_mask\"]:\n",
    "            collated[key] = pad_sequence(tensors, batch_first=True, padding_value=0)\n",
    "        else:\n",
    "            collated[key] = torch.stack(tensors)\n",
    "    return collated, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbf2f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleCLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, mode=\"both\"):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.mode = mode\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.clip.config.projection_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3)  # 3 classes: Low, Medium, High\n",
    "        )\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        if self.mode == \"both\":\n",
    "            text_features = self.clip.get_text_features(\n",
    "                input_ids=kwargs[\"input_ids\"], attention_mask=kwargs[\"attention_mask\"]\n",
    "            )\n",
    "            image_features = self.clip.get_image_features(pixel_values=kwargs[\"pixel_values\"])\n",
    "            embedding = (text_features + image_features) / 2\n",
    "        elif self.mode == \"image\":\n",
    "            embedding = self.clip.get_image_features(pixel_values=kwargs[\"pixel_values\"])\n",
    "        elif self.mode == \"text\":\n",
    "            embedding = self.clip.get_text_features(\n",
    "                input_ids=kwargs[\"input_ids\"], attention_mask=kwargs[\"attention_mask\"]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        return self.classifier(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f2afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split & Load Data\n",
    "df_train, df_val = train_test_split(df_all, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = HatefulMemeDataset(df_train, os.path.join(dataset_dir, \"img\"), clip_processor, mode=input_mode)\n",
    "val_dataset = HatefulMemeDataset(df_val, os.path.join(dataset_dir, \"img\"), clip_processor, mode=input_mode)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=clip_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=clip_collate_fn)\n",
    "\n",
    "#  Load Model + Weighted Loss\n",
    "model = FlexibleCLIPClassifier(clip_model, mode=input_mode).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffec2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "# Compute class weights (FIXED)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=df_all[\"harm_label\"]\n",
    ")\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76cb3927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ€ Starting Epoch 1/3\n",
      "  Batch 10 | Loss: 1.2326\n",
      "  Batch 20 | Loss: 1.1274\n",
      "  Batch 30 | Loss: 0.8664\n",
      "  Batch 40 | Loss: 0.7557\n",
      "  Batch 50 | Loss: 0.9154\n",
      "  Batch 60 | Loss: 1.8435\n",
      "  Batch 70 | Loss: 1.4652\n",
      "  Batch 80 | Loss: 0.9020\n",
      "  Batch 90 | Loss: 0.8894\n",
      "  Batch 100 | Loss: 0.7776\n",
      "  Batch 110 | Loss: 1.9605\n",
      "  Batch 120 | Loss: 0.8285\n",
      "  Batch 130 | Loss: 1.0702\n",
      "  Batch 140 | Loss: 0.7007\n",
      "  Batch 150 | Loss: 0.7042\n",
      "  Batch 160 | Loss: 0.8022\n",
      "  Batch 170 | Loss: 0.8982\n",
      "  Batch 180 | Loss: 0.6535\n",
      "  Batch 190 | Loss: 1.0750\n",
      "  Batch 200 | Loss: 0.7981\n",
      "  Batch 210 | Loss: 1.2212\n",
      "  Batch 220 | Loss: 0.6875\n",
      "  Batch 230 | Loss: 0.8044\n",
      "  Batch 240 | Loss: 0.7472\n",
      "  Batch 250 | Loss: 0.6844\n",
      "  Batch 260 | Loss: 0.8814\n",
      "  Batch 270 | Loss: 1.1361\n",
      "  Batch 280 | Loss: 0.8956\n",
      "  Batch 290 | Loss: 0.7338\n",
      "  Batch 300 | Loss: 1.2684\n",
      "  Batch 310 | Loss: 0.9462\n",
      "  Batch 320 | Loss: 0.7824\n",
      "  Batch 330 | Loss: 0.7941\n",
      "  Batch 340 | Loss: 0.9402\n",
      "  Batch 350 | Loss: 0.9022\n",
      "  Batch 360 | Loss: 0.9097\n",
      "  Batch 370 | Loss: 0.6923\n",
      "  Batch 380 | Loss: 0.8869\n",
      "  Batch 390 | Loss: 0.7654\n",
      "  Batch 400 | Loss: 0.8747\n",
      "  Batch 410 | Loss: 1.0295\n",
      "  Batch 420 | Loss: 0.7991\n",
      "  Batch 430 | Loss: 0.6927\n",
      "  Batch 440 | Loss: 0.8327\n",
      "  Batch 450 | Loss: 0.5998\n",
      " Epoch 1 Complete - Total Loss: 426.8767\n",
      "\n",
      "ðŸŒ€ Starting Epoch 2/3\n",
      "  Batch 10 | Loss: 0.7138\n",
      "  Batch 20 | Loss: 1.1859\n",
      "  Batch 30 | Loss: 0.9582\n",
      "  Batch 40 | Loss: 0.6069\n",
      "  Batch 50 | Loss: 0.8332\n",
      "  Batch 60 | Loss: 0.6155\n",
      "  Batch 70 | Loss: 0.7762\n",
      "  Batch 80 | Loss: 0.7821\n",
      "  Batch 90 | Loss: 0.5714\n",
      "  Batch 100 | Loss: 0.5660\n",
      "  Batch 110 | Loss: 0.8419\n",
      "  Batch 120 | Loss: 0.4674\n",
      "  Batch 130 | Loss: 0.9922\n",
      "  Batch 140 | Loss: 0.9102\n",
      "  Batch 150 | Loss: 0.7198\n",
      "  Batch 160 | Loss: 0.9844\n",
      "  Batch 170 | Loss: 0.5075\n",
      "  Batch 180 | Loss: 0.4279\n",
      "  Batch 190 | Loss: 0.4381\n",
      "  Batch 200 | Loss: 0.5713\n",
      "  Batch 210 | Loss: 0.9111\n",
      "  Batch 220 | Loss: 0.4974\n",
      "  Batch 230 | Loss: 0.8834\n",
      "  Batch 240 | Loss: 0.7114\n",
      "  Batch 250 | Loss: 0.7077\n",
      "  Batch 260 | Loss: 0.8745\n",
      "  Batch 270 | Loss: 0.7359\n",
      "  Batch 280 | Loss: 0.8596\n",
      "  Batch 290 | Loss: 0.6234\n",
      "  Batch 300 | Loss: 0.5711\n",
      "  Batch 310 | Loss: 0.3219\n",
      "  Batch 320 | Loss: 1.0598\n",
      "  Batch 330 | Loss: 0.6111\n",
      "  Batch 340 | Loss: 1.0170\n",
      "  Batch 350 | Loss: 0.8870\n",
      "  Batch 360 | Loss: 1.1737\n",
      "  Batch 370 | Loss: 0.6678\n",
      "  Batch 380 | Loss: 1.8973\n",
      "  Batch 390 | Loss: 0.4825\n",
      "  Batch 400 | Loss: 0.6881\n",
      "  Batch 410 | Loss: 1.4821\n",
      "  Batch 420 | Loss: 1.0514\n",
      "  Batch 430 | Loss: 0.7192\n",
      "  Batch 440 | Loss: 0.8093\n",
      "  Batch 450 | Loss: 1.8617\n",
      " Epoch 2 Complete - Total Loss: 355.1055\n",
      "\n",
      "ðŸŒ€ Starting Epoch 3/3\n",
      "  Batch 10 | Loss: 0.7225\n",
      "  Batch 20 | Loss: 0.3470\n",
      "  Batch 30 | Loss: 0.5833\n",
      "  Batch 40 | Loss: 0.5784\n",
      "  Batch 50 | Loss: 1.2496\n",
      "  Batch 60 | Loss: 0.3466\n",
      "  Batch 70 | Loss: 0.4159\n",
      "  Batch 80 | Loss: 0.4920\n",
      "  Batch 90 | Loss: 0.3857\n",
      "  Batch 100 | Loss: 0.1800\n",
      "  Batch 110 | Loss: 0.5831\n",
      "  Batch 120 | Loss: 0.8933\n",
      "  Batch 130 | Loss: 0.6390\n",
      "  Batch 140 | Loss: 0.3007\n",
      "  Batch 150 | Loss: 0.3355\n",
      "  Batch 160 | Loss: 1.1416\n",
      "  Batch 170 | Loss: 0.8773\n",
      "  Batch 180 | Loss: 0.2969\n",
      "  Batch 190 | Loss: 0.4579\n",
      "  Batch 200 | Loss: 0.6077\n",
      "  Batch 210 | Loss: 1.0274\n",
      "  Batch 220 | Loss: 0.7468\n",
      "  Batch 230 | Loss: 0.5293\n",
      "  Batch 240 | Loss: 0.6938\n",
      "  Batch 250 | Loss: 0.8576\n",
      "  Batch 260 | Loss: 0.8356\n",
      "  Batch 270 | Loss: 0.3946\n",
      "  Batch 280 | Loss: 0.4404\n",
      "  Batch 290 | Loss: 0.7536\n",
      "  Batch 300 | Loss: 0.8020\n",
      "  Batch 310 | Loss: 0.8799\n",
      "  Batch 320 | Loss: 2.2727\n",
      "  Batch 330 | Loss: 0.4754\n",
      "  Batch 340 | Loss: 0.3839\n",
      "  Batch 350 | Loss: 1.3595\n",
      "  Batch 360 | Loss: 0.7226\n",
      "  Batch 370 | Loss: 0.6045\n",
      "  Batch 380 | Loss: 0.5402\n",
      "  Batch 390 | Loss: 0.7971\n",
      "  Batch 400 | Loss: 0.4656\n",
      "  Batch 410 | Loss: 0.6502\n",
      "  Batch 420 | Loss: 0.9723\n",
      "  Batch 430 | Loss: 0.5022\n",
      "  Batch 440 | Loss: 0.8050\n",
      "  Batch 450 | Loss: 0.9693\n",
      " Epoch 3 Complete - Total Loss: 316.2956\n"
     ]
    }
   ],
   "source": [
    "#  Training\n",
    "def train_model(model, dataloader, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        print(f\"\\nðŸŒ€ Starting Epoch {epoch+1}/{epochs}\")\n",
    "        for i, (batch) in enumerate(dataloader):\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"  Batch {i+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\" Epoch {epoch+1} Complete - Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6832d204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.71      0.78      0.74       124\n",
      "      Medium       0.53      0.44      0.48        64\n",
      "        High       0.50      0.42      0.45        12\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.58      0.55      0.56       200\n",
      "weighted avg       0.64      0.65      0.64       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"\\nðŸ“Š Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"Low\", \"Medium\", \"High\"]))\n",
    "\n",
    "evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3014d45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to full_model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"full_new_model.pt\")\n",
    "print(\"âœ… Model saved to full_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
